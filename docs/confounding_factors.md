# Confounding factors workshop

The aim of this workshop is to get the students to think of how they are going to run their study in a way that makes it consistent (replicable) when they run it as different individuals.

## Activity 1

This activity is designed to get them thinking about potential confounding variables in an experiment. We discussed the first 4 problems in class (but that doesn’t mean that all students understood these points). The remaining ones were not discussed.
In particular, the activity is designed to get them to think about experimental design (counterbalancing, randomising), but also potential biases that can creep in from different experimenters, e.g. different instructions, different presentation of materials, different scoring protocols etc.

Here are some model answers: the first 4 were discussed in class.

- Materials (different lists might not be matched)
- Different experimenters (potential biases in how friendly / formal / supportive they are, etc.)
- Different instructions 1 (tweaking! – if the instructions are tweaked, then there is a confound)
- Different instructions 2 (each group design their own materials / instructions)
- Demand characteristics (hypothesis is known so participants may try to give the "correct" response)
- Selection effects (1) Different groups of students available on different days, e.g. maybe Stage 1 students are available on Tuesdays, and MSc students on Thursdays.
- Selection effects (2) Different groups of students may choose different conditions, e.g. maybe people who are nervous don't want to have to close their eyes.
- Time of day effects (morning vs afternoon)
- (Day of the week effects – Tuesday vs Thursday – see point above about selection effects, but there could be a day of the week effect, such as not testing after a big student drinking night)

- Potential Extra (hidden) factors
  - How are materials presented by each group (spoken / written / timing issues etc)?
  - How are responses scored by each group (What counts as an error)?

## Activity 2

The aim of this activity is to get them to apply what they have learned to their own project. In particular, we would like to get them to agree a set of instructions that they will use to:

1. Introduce and run the study with participants. This can’t just be the brief. It has to include things like welcoming participants to the study, asking them to turn off their phones, and asking whether they have any questions before they begin. Have they agreed what responses they will give to such questions? (e.g. What will they say if a participant asks for a break mid way through the experiment)?

1. It should also cover basic details like where they are going to do the testing (in a laboratory, in people's houses, in the library etc.). Will the participant be alone, or will they test several people at once (etc).

1. It should also have clear instructions as to how the data will be scored. What constitutes a valid response, or an error, for instance? What will they do with missing values (in questionnaires)?

## Activity 3

The aim of this activity is to think about the data they will generate, and to think about the steps they might take to improve the likely quality of their data.

### Experimental designs

The obvious thing to look at is whether they are likely to get floor / ceiling effects. What if people recall all the words, make zero errors etc.?

Another issue is how they deal with ambiguous responses (poor handwriting, partially correct responses etc).

An advanced issue is how they can improve the reliability of their outcome measure. For example, if a group plan to test recall from 10 items - why not run it twice (with different lists), and average the score? It is a less noisy measure, and will only take an extra minute or two. (Even better, they could check the reliability of their outcome measure, by correlating the two scores).

### Correlational designs

The main thing to think about is whether their data will be appropriate for a correlational analysis. i.e. are they getting a continuous range of scores on both variables, with no outliers, no gaps in the data (etc)? How will they test for these things? (We will introduce them to a spreadsheet to check for these things in a later workshop, but for now, we want them to think about it). These problems may arise if they are using their own questionnaire.

The other thing to think about is what they will do with incomplete questionnaire responses, e.g. people missing questions they don’t want to answer. They can’t just drop that score from the scale score, because it won’t be appropriate.
